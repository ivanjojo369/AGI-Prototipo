{"type": "job_meta", "status": "running", "meta": {"goal": "demo", "params": {}}, "ts": "2025-09-15T18:10:09.684312Z"}
{"step_id": "m1", "status": "running", "analysis": {"id": "m1", "kind": "action", "name": "memory_search", "inputs": {"query": "demo", "k": 3}, "preconditions": [], "postconditions": [], "retries": 0, "continue_on_error": false}, "result": null, "error": null, "meta": null, "type": "step_start", "steps": null, "ts": "2025-09-15T18:11:24.184322Z"}
{"step_id": "m1", "status": "completed", "analysis": null, "result": [{"text": "Recuerdo 5", "score": 0.0039606538148881446, "confidence": 1.0, "citation_id": "mem://b2578417-54fd-4b79-9ab6-7272b13ebb45", "metadata": [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, "…"], "result_quality": "unknown", "trace_id": "70668f5a-d928-4842-a114-8d3dad2b4135"}, {"text": "Recuerdo 14", "score": 0.0039577740009347296, "confidence": 1.0, "citation_id": "mem://25e769f9-dd8d-4b71-a4ec-6da78bac90a5", "metadata": [14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, "…"], "result_quality": "unknown", "trace_id": "d594d688-6222-4a08-a351-516fddb87e8a"}, {"text": "Recuerdo 42", "score": 0.003931417826942629, "confidence": 1.0, "citation_id": "mem://e1b31313-7aec-40ac-af88-047206365b85", "metadata": [42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, "…"], "result_quality": "unknown", "trace_id": "b26b35fc-b666-4a9b-b127-52243e6e78da"}], "error": null, "meta": null, "type": "step_end", "steps": null, "ts": "2025-09-15T18:11:24.186818Z"}
{"step_id": "s1", "status": "running", "analysis": {"id": "s1", "kind": "action", "name": "python_exec", "inputs": {"code": "result = 40+2"}, "preconditions": [], "postconditions": ["result == 42"], "retries": 0, "continue_on_error": false}, "result": null, "error": null, "meta": null, "type": "step_start", "steps": null, "ts": "2025-09-15T18:11:24.193880Z"}
{"step_id": "s1", "status": "completed", "analysis": null, "result": 42, "error": null, "meta": null, "type": "step_end", "steps": null, "ts": "2025-09-15T18:11:24.195106Z"}
{"step_id": "r1", "status": "running", "analysis": {"id": "r1", "kind": "action", "name": "filesystem_read", "inputs": {"path": "server.py"}, "preconditions": [], "postconditions": ["len(result) > 0"], "retries": 0, "continue_on_error": false}, "result": null, "error": null, "meta": null, "type": "step_start", "steps": null, "ts": "2025-09-15T18:11:24.197464Z"}
{"step_id": "r1", "status": "completed", "analysis": null, "result": "# server.py — AGI Prototype (Fases 1–4: + Jobs, job_id en /plan/execute)\nfrom __future__ import annotations\n\nimport os\nimport statistics\nfrom time import perf_counter\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom uuid import uuid4\n\nfrom fastapi import Body, FastAPI, HTTPException, Request, Security\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.openapi.utils import get_openapi\nfrom fastapi.responses import JSONResponse\nfrom fastapi.security import APIKeyHeader\n\n# ──────────────────────────────────────────────────────────────────────────────\n# Routers / Módulos internos\n# ──────────────────────────────────────────────────────────────────────────────\n# Router de Jobs (Fase 4)\nfrom server_jobs_router import jobs_router\n\n# Memoria vectorial unificada\nfrom memory.unified_memory import UnifiedMemory\n\n# Orquestador (fallback si no existe paquete reasoner)\ntry:\n    from reasoner.orchestrator import SkillOrchestrator  # type: ignore\nexcept Exception:\n    from orchestrator import SkillOrchestrator  # type: ignore\n\n# Planner HTN y acciones builtin\nfrom planner.task_planner import (\n    Plan as HTNPlan,\n    Step as HTNStep,\n    TaskPlanner,\n    BuiltinActions,\n)\n\n# Curriculum (con fallback mínimo)\ntry:\n    from reasoner.curriculum import CurriculumBuilder  # type: ignore\nexcept Exception:\n\n    class CurriculumBuilder:  # pragma: no cover\n        def build(self, failures: List[Dict[str, Any]], max_items: int = 10) -> List[Dict[str, Any]]:\n            out = []\n            for i, fail in enumerate(failures[:max_items], 1):\n                q = fail.get(\"query\") or fail.get(\"input\") or f\"tarea_{i}\"\n                out.append(\n                    {\"id\": str(i), \"practice\": f\"Reintenta: {q}\", \"hint\": \"Divide y valida.\", \"tags\": [\"autogen\"]}\n                )\n            return out\n\n\n# ──────────────────────────────────────────────────────────────────────────────\n# App & Seguridad (API Key)\n# ──────────────────────────────────────────────────────────────────────────────\nAPI_KEY_HEADER_NAME = \"x-api-key\"\napi_key_header = APIKeyHeader(name=API_KEY_HEADER_NAME, auto_error=False)\n\napp = FastAPI(title=\"AGI Prototype (FAISS-CPU)\", version=\"4.1.0\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Incluir router de Jobs (requiere API_KEY configurada para usarse)\napp.include_router(jobs_router)\n\n# Whitelist para docs/health sin API key\nDOCS_WHITELIST = {\"/\", \"/docs\", \"/redoc\", \"/openapi.json\", \"/status\", \"/get_status\"}\n\n\n@app.middleware(\"http\")\nasync def api_key_middleware(request: Request, call_next):\n    # Bypass en tests/CI controlado\n    if os.getenv(\"PYTEST_CURRENT_TEST\") or os.getenv(\"ALLOW_TEST_NO_AUTH\") == \"1\":\n        return await call_next(request)\n\n    required_key = (os.getenv(\"API_KEY\") or \"\").strip()\n    if not required_key or request.url.path in DOCS_WHITELIST:\n        return await call_next(request)\n\n    sent_key = request.headers.get(API_KEY_HEADER_NAME)\n    # Permite Bearer como alternativa\n    if not sent_key:\n        auth = request.headers.get(\"authorization\", \"\")\n        if auth.lower().startswith(\"bearer \"):\n            sent_key = auth[7:].strip()\n\n    if sent_key != required_key:\n        return JSONResponse({\"detail\": \"Forbidden: invalid or missing API Key\"}, status_code=403)\n\n    return await call_next(request)\n\n\n# OpenAPI con esquema de seguridad por header\ndef custom_openapi() -> dict:\n    if app.openapi_schema:\n        return app.openapi_schema\n    schema = get_openapi(\n        title=app.title,\n        version=app.version,\n        description=\"AGI endpoints (Fases 1–4, incluye /jobs/*)\",\n        routes=app.routes,\n    )\n    schema.setdefault(\"components\", {}).setdefault(\"securitySchemes\", {})[\"ApiKeyAuth\"] = {\n        \"type\": \"apiKey\",\n        \"in\": \"header\",\n        \"name\": API_KEY_HEADER_NAME,\n        \"description\": \"Introduce tu API Key.\",\n    }\n    schema[\"security\"] = [{\"ApiKeyAuth\": []}]\n    app.openapi_schema = schema\n    return app.openapi_schema\n\n\napp.openapi = custom_openapi  # type: ignore[assignment]\n\n# ──────────────────────────────────────────────────────────────────────────────\n# Estado / Utilidades\n# ──────────────────────────────────────────────────────────────────────────────\nUM = UnifiedMemory(memory_dir=\"memory_store\", use_gpu=False)\n\n\ndef _vector_provider(query: str, k: int) -> List[Dict[str, Any]]:\n    # CONF_SCALE controla cómo escalamos el score a 'confidence'\n    scale = float(os.environ.get(\"CONF_SCALE\", \"200.0\"))\n    try:\n        res = UM.retrieve_relevant_memories(query, top_k=k)\n    except Exception as e:  # pragma: no cover\n        return [{\"text\": f\"[vector error] {type(e).__name__}: {e}\", \"score\": 0.0, \"confidence\": 0.0}]\n    out: List[Dict[str, Any]] = []\n    for r in res:\n        raw = float(r.get(\"score\", 0.0))\n        conf = max(0.0, min(1.0, raw * scale))\n        meta = r.get(\"metadata\", {})\n        if isinstance(meta, list) and len(meta) > 20:\n            meta = meta[:20] + [\"…\"]\n        out.append(\n            {\n                \"text\": r.get(\"text\", \"\"),\n                \"score\": raw,\n                \"confidence\": conf,\n                \"citation_id\": r.get(\"citation_id\") or r.get(\"mem_id\"),\n                \"metadata\": meta,\n                \"result_quality\": r.get(\"result_quality\", \"unknown\"),\n                \"trace_id\": r.get(\"trace_id\"),\n            }\n        )\n    return out\n\n\nORCH = SkillOrchestrator(\n    memory_vector_provider=_vector_provider,\n    fs_root=\".\",\n    score_threshold=0.45,\n)\n\n\ndef memory_action(**kwargs):\n    q = (kwargs.get(\"query\") or \"\").strip()\n    k = int(kwargs.get(\"k\", 5))\n    return _vector_provider(q, k)\n\n\nactions = {\n    \"python_exec\": BuiltinActions.python_exec,\n    \"filesystem_read\": BuiltinActions.filesystem_read,\n    \"memory_search\": memory_action,\n    \"memory_vector\": memory_action,  # alias\n    \"search_web\": BuiltinActions.search_web,\n}\n\nPLANNER = TaskPlanner(actions=actions, curriculum_path=\"data/curriculum/planner_curriculum.jsonl\")\nCURRICULUM = CurriculumBuilder()\n\n# ──────────────────────────────────────────────────────────────────────────────\n# Métricas\n# ──────────────────────────────────────────────────────────────────────────────\nCOUNTERS: Dict[str, int] = {\n    k: 0\n    for k in [\n        \"total_requests\",\n        \"chat_requests\",\n        \"upserts_total\",\n        \"search_requests\",\n        \"vec_upserts_total\",\n        \"vec_search_requests\",\n        \"reason_requests\",\n        \"plan_requests\",\n        \"curriculum_requests\",\n    ]\n}\nLAT_LOG: Dict[str, List[float]] = {\n    k: []\n    for k in [\"chat\", \"upsert\", \"search\", \"vec_upsert\", \"vec_search\", \"reason\", \"plan\", \"curriculum\"]\n}\n\n\ndef _record_latency(bucket: str, t0: float) -> None:\n    LAT_LOG.setdefault(bucket, []).append((perf_counter() - t0) * 1000.0)\n\n\ndef _avg(x: List[float]) -> Optional[float]:\n    return float(statistics.fmean(x)) if x else None\n\n\ndef _p95(x: List[float]) -> Optional[float]:\n    if not x:\n        return None\n    s = sorted(x)\n    from math import ceil\n\n    return float(s[max(0, ceil(0.95 * len(s)) - 1)])\n\n\n# ──────────────────────────────────────────────────────────────────────────────\n# Memoria semántica (RAM) de compatibilidad — Fase 1\n# ──────────────────────────────────────────────────────────────────────────────\nSTORE: List[Dict[str, Any]] = []\n\n\ndef _simple_score(q: str, t: str) -> float:\n    if not q or not t:\n        return 0.0\n    ql, tl = q.lower(), t.lower()\n    hits = tl.count(ql)\n    if hits <= 0:\n        return 0.0\n    return min(1.0, hits / max(1, len(tl) / max(1, len(ql))))\n\n\n# ──────────────────────────────────────────────────────────────────────────────\n# Endpoints\n# ──────────────────────────────────────────────────────────────────────────────\n@app.get(\"/\")\ndef health() -> Dict[str, Any]:\n    return {\"ok\": True}\n\n\n@app.get(\"/status\")\ndef status_alias() -> Dict[str, Any]:\n    return get_status()\n\n\n@app.get(\"/get_status\")\ndef get_status() -> Dict[str, Any]:\n    try:\n        um_status = UM.get_status() if hasattr(UM, \"get_status\") else {}\n    except Exception as e:  # pragma: no cover\n        um_status = {\"ok\": False, \"error\": type(e).__name__, \"msg\": str(e)}\n    index_size_vec = 0\n    if isinstance(um_status, dict):\n        for k in (\"index_size\", \"ntotal\", \"size\", \"count\"):\n            v = um_status.get(k)\n            if isinstance(v, (int, float)):\n                index_size_vec = int(v)\n                break\n    return {\n        \"ok\": True,\n        \"counters\": {**COUNTERS, \"index_size_semantic\": len(STORE), \"index_size_vector\": index_size_vec},\n        \"latency_ms\": {\n            \"chat_avg\": _avg(LAT_LOG[\"chat\"]),\n            \"chat_p95\": _p95(LAT_LOG[\"chat\"]),\n            \"upsert_avg\": _avg(LAT_LOG[\"upsert\"]),\n            \"upsert_p95\": _p95(LAT_LOG[\"upsert\"]),\n            \"search_avg\": _avg(LAT_LOG[\"search\"]),\n            \"search_p95\": _p95(LAT_LOG[\"search\"]),\n            \"vec_upsert_avg\": _avg(LAT_LOG[\"vec_upsert\"]),\n            \"vec_upsert_p95\": _p95(LAT_LOG[\"vec_upsert\"]),\n            \"vec_search_avg\": _avg(LAT_LOG[\"vec_search\"]),\n            \"vec_search_p95\": _p95(LAT_LOG[\"vec_search\"]),\n            \"reason_avg\": _avg(LAT_LOG[\"reason\"]),\n            \"reason_p95\": _p95(LAT_LOG[\"reason\"]),\n            \"plan_avg\": _avg(LAT_LOG[\"plan\"]),\n            \"plan_p95\": _p95(LAT_LOG[\"plan\"]),\n            \"curriculum_avg\": _avg(LAT_LOG[\"curriculum\"]),\n            \"curriculum_p95\": _p95(LAT_LOG[\"curriculum\"]),\n        },\n        \"vector_status\": um_status,\n    }\n\n\n@app.get(\"/auth/echo\", dependencies=[Security(api_key_header)])\ndef auth_echo() -> Dict[str, Any]:\n    return {\"ok\": True, \"auth\": \"passed\"}\n\n\n# ------------------------------ Chat -----------------------------------------#\n@app.post(\"/chat\")\nasync def chat(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:\n    t0 = perf_counter()\n    try:\n        COUNTERS[\"total_requests\"] += 1\n        COUNTERS[\"chat_requests\"] += 1\n        msgs = payload.get(\"messages\", []) or []\n        user_msg = next((m.get(\"content\", \"\") for m in reversed(msgs) if m.get(\"role\") == \"user\"), \"\").strip()\n        mode = (payload.get(\"mode\") or \"echo\").lower()\n        k = int(payload.get(\"k\", 3))\n\n        if mode == \"reason\":\n            return {\"ok\": True, \"query\": user_msg, **ORCH.execute(user_msg, k=k)}\n\n        if mode == \"plan\":\n            plan = _solve_goal_to_plan(user_msg, payload.get(\"context\") or {})\n            return {\"ok\": True, \"skill\": \"task_planner\", \"goal\": user_msg, \"plan\": plan.model_dump()}\n\n        if mode == \"curriculum\":\n            failures = payload.get(\"failures\") or []\n            return {\n                \"ok\": True,\n                \"skill\": \"curriculum\",\n                \"items\": CURRICULUM.build(failures, max_items=int(payload.get(\"max\", 10))),\n            }\n\n        return {\"text\": f\"AGI: {user_msg} <|end_of_turn|>\"}\n    finally:\n        _record_latency(\"chat\", t0)\n\n\n# --------------- Memoria semántica (RAM) -------------------------------------#\n@app.post(\"/memory/semantic/upsert\")\nasync def semantic_upsert(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:\n    t0 = perf_counter()\n    try:\n        COUNTERS[\"total_requests\"] += 1\n        facts = payload.get(\"facts\", []) or []\n        req_trace = payload.get(\"trace_id\") or str(uuid4())\n        upserted, mem_ids = 0, []\n        for f in facts:\n            text = (f or {}).get(\"text\")\n            if not text:\n                continue\n            mem_id = f.get(\"mem_id\") or f\"mem://{uuid4()}\"\n            STORE.append(\n                {\n                    \"mem_id\": mem_id,\n                    \"doc_id\": f.get(\"doc_id\") or mem_id,\n                    \"text\": text,\n                    \"meta\": f.get(\"meta\") or {},\n                    \"result_quality\": f.get(\"result_quality\", \"unknown\"),\n                    \"confidence\": float(f.get(\"confidence\", 0.5)),\n                    \"trace_id\": f.get(\"trace_id\") or req_trace,\n                }\n            )\n            mem_ids.append(mem_id)\n            upserted += 1\n        COUNTERS[\"upserts_total\"] += upserted\n        return {\"ok\": True, \"upserted\": upserted, \"trace_id\": req_trace, \"mem_ids\": mem_ids}\n    finally:\n        _record_latency(\"upsert\", t0)\n\n\n@app.post(\"/memory/semantic/search\")\nasync def semantic_search(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:\n    t0 = perf_counter()\n    try:\n        COUNTERS[\"total_requests\"] += 1\n        COUNTERS[\"search_requests\"] += 1\n        q = (payload.get(\"q\") or \"\").strip()\n        k = int(payload.get(\"k\", 5))\n        scored: List[Tuple[float, Dict[str, Any]]] = [(_simple_score(q, f.get(\"text\", \"\")), f) for f in STORE]\n        scored.sort(key=lambda x: x[0], reverse=True)\n        out = []\n        for s, f in scored[:k]:\n            row = dict(f)\n            row[\"score\"] = float(s)\n            row[\"confidence_source\"] = row.get(\"confidence\", 0.5)\n            row[\"confidence\"] = float(s)\n            row[\"citation_id\"] = f.get(\"mem_id\") or f.get(\"doc_id\")\n            out.append(row)\n        return {\"ok\": True, \"results\": out, \"query\": q, \"k\": k}\n    finally:\n        _record_latency(\"search\", t0)\n\n\n# ------------------------- Memoria vectorial (UM) -----------------------------#\n@app.post(\"/memory/vector/upsert\")\nasync def vector_upsert(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:\n    t0 = perf_counter()\n    try:\n        COUNTERS[\"total_requests\"] += 1\n        facts = payload.get(\"facts\", []) or []\n        req_trace = payload.get(\"trace_id\")\n        upserted, mem_ids = 0, []\n        for f in facts:\n            text = (f or {}).get(\"text\")\n            if not text:\n                continue\n            meta = f.get(\"metadata\") or f.get(\"meta\") or {}\n            res = UM.add_to_vector_memory(\n                text,\n                metadata=meta,\n                result_quality=f.get(\"result_quality\", \"unknown\"),\n                confidence=float(f.get(\"confidence\", 0.5)),\n                trace_id=f.get(\"trace_id\") or req_trace,\n            )\n            mem_id = (\n                res[1]\n                if isinstance(res, tuple) and len(res) >= 2\n                else (res.get(\"mem_id\") if isinstance(res, dict) else (res if isinstance(res, str) else None))\n            )\n            if mem_id:\n                mem_ids.append(mem_id)\n                upserted += 1\n        COUNTERS[\"vec_upserts_total\"] += upserted\n        return {\"ok\": True, \"upserted\": upserted, \"mem_ids\": mem_ids}\n    finally:\n        _record_latency(\"vec_upsert\", t0)\n\n\n@app.post(\"/memory/vector/search\")\nasync def vector_search(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:\n    t0 = perf_counter()\n    try:\n        COUNTERS[\"total_requests\"] += 1\n        COUNTERS[\"vec_search_requests\"] += 1\n        q = (payload.get(\"q\") or \"\").strip()\n        k = int(payload.get(\"k\", 5))\n        return {\"ok\": True, \"results\": _vector_provider(q, k), \"query\": q, \"k\": k}\n    finally:\n        _record_latency(\"vec_search\", t0)\n\n\n# ----------------------------- Razonamiento ----------------------------------#\n@app.post(\"/reason/execute\")\nasync def reason_execute(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:\n    t0 = perf_counter()\n    try:\n        COUNTERS[\"total_requests\"] += 1\n        COUNTERS[\"reason_requests\"] += 1\n        q = (payload.get(\"query\") or \"\").strip()\n        k = int(payload.get(\"k\", 5))\n        return ORCH.execute(q, k=k)\n    finally:\n        _record_latency(\"reason\", t0)\n\n\n# ------------------------------ Planner (HTN) --------------------------------#\ndef _normalize_goal_text(goal: Any) -> str:\n    if isinstance(goal, list):\n        s = \" \".join(str(x) for x in goal)\n    elif isinstance(goal, str):\n        s = goal\n    else:\n        s = str(goal)\n    return s\n\n\ndef _solve_goal_to_plan(goal: Any, context: Dict[str, Any]) -> HTNPlan:\n    # Heurística simple compatible con tu Fase 3\n    goal_str = _normalize_goal_text(goal)\n    gl = goal_str.lower()\n    if \"buscar\" in gl:\n        return HTNPlan(\n            goal=goal_str,\n            steps=[\n                HTNStep(\n                    id=\"T\",\n                    kind=\"task\",\n                    name=\"buscar_y_leer\",\n                    inputs={\"query\": context.get(\"query\", goal_str), \"path\": context.get(\"path\", \"server.py\")},\n                )\n            ],\n            metadata={\"auto\": True},\n        )\n    if \"leer\" in gl or \"read\" in gl:\n        return HTNPlan(\n            goal=goal_str,\n            steps=[\n                HTNStep(\n                    id=\"r1\",\n                    kind=\"action\",\n                    name=\"filesystem_read\",\n                    inputs={\"path\": context.get(\"path\", \"server.py\")},\n                    postconditions=[\"len(result) > 0\"],\n                )\n            ],\n            metadata={\"auto\": True},\n        )\n    # Default: 3 pasos (mem → py → fs)\n    return HTNPlan(\n        goal=goal_str,\n        steps=[\n            HTNStep(id=\"m1\", kind=\"action\", name=\"memory_search\", inputs={\"query\": goal_str, \"k\": 3}),\n            HTNStep(id=\"s1\", kind=\"action\", name=\"python_exec\", inputs={\"code\": \"result = 40+2\"}, postconditions=[\"result == 42\"]),\n            HTNStep(\n                id=\"r1\",\n                kind=\"action\",\n                name=\"filesystem_read\",\n                inputs={\"path\": context.get(\"path\", \"server.py\")},\n                postconditions=[\"len(result) > 0\"],\n            ),\n        ],\n        metadata={\"auto\": True},\n    )\n\n\nfrom pydantic import BaseModel, Field\n\n\nclass PlanExecuteRequest(BaseModel):\n    plan: HTNPlan = Field(...)\n    context: Dict[str, Any] = Field(default_factory=dict)\n    # NUEVO: permitir que el cliente encadene el job_id (para checkpoints automáticos)\n    job_id: Optional[str] = None\n\n\nclass PlanExecuteResponse(BaseModel):\n    status: str\n    goal: str\n    results: Dict[str, Any]\n    errors: list\n    curriculum_entries: int\n    confidence: float\n\n\n@app.post(\"/plan/solve\")\nasync def plan_solve(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:\n    t0 = perf_counter()\n    try:\n        COUNTERS[\"total_requests\"] += 1\n        COUNTERS[\"plan_requests\"] += 1\n        goal = payload.get(\"goal\")\n        if goal is None or (isinstance(goal, str) and not goal.strip()):\n            raise HTTPException(400, \"goal requerido\")\n        plan = _solve_goal_to_plan(goal, payload.get(\"context\") or {})\n        return {\"ok\": True, \"goal\": _normalize_goal_text(goal), \"plan\": plan.model_dump()}\n    finally:\n        _record_latency(\"plan\", t0)\n\n\n@app.post(\"/plan/execute\", response_model=PlanExecuteResponse, tags=[\"planner\"])\nasync def plan_execute(req: PlanExecuteRequest) -> Dict[str, Any]:\n    t0 = perf_counter()\n    try:\n        COUNTERS[\"total_requests\"] += 1\n        COUNTERS[\"plan_requests\"] += 1\n        # Propaga job_id al planner para que emita checkpoints (plan/step_start/step_end/status)\n        return PLANNER.execute_plan(req.plan, context=req.context, job_id=req.job_id)\n    finally:\n        _record_latency(\"plan\", t0)\n\n\n@app.post(\"/curriculum/build\")\nasync def curriculum_build(payload: Dict[str, Any] = Body(...)) -> Dict[str, Any]:\n    t0 = perf_counter()\n    try:\n        COUNTERS[\"total_requests\"] += 1\n        COUNTERS[\"curriculum_requests\"] += 1\n        failures = payload.get(\"failures\") or []\n        max_items = int(payload.get(\"max\", 10))\n        items = CURRICULUM.build(failures, max_items=max_items)\n        return {\"ok\": True, \"items\": items, \"count\": len(items)}\n    finally:\n        _record_latency(\"curriculum\", t0)\n\n\n# ──────────────────────────────────────────────────────────────────────────────\n# Main\n# ──────────────────────────────────────────────────────────────────────────────\nif __name__ == \"__main__\":\n    import uvicorn\n\n    port = int(os.environ.get(\"PORT\", \"8010\"))\n    # Log rápido: API key enmascarada\n    ak = (os.getenv(\"API_KEY\") or \"\")\n    mask = (\"*\" * (len(ak) - 4) + ak[-4:]) if ak else \"(none)\"\n    print(f\"[server] API_KEY={mask}  http://127.0.0.1:{port}\")\n    uvicorn.run(\"server:app\", host=\"127.0.0.1\", port=port, reload=True)\n", "error": null, "meta": null, "type": "step_end", "steps": null, "ts": "2025-09-15T18:11:24.199696Z"}
{"step_id": null, "status": "completed", "analysis": null, "result": null, "error": null, "meta": null, "type": "status", "steps": null, "ts": "2025-09-15T18:11:24.203044Z"}
{"step_id": null, "status": "running", "analysis": null, "result": null, "error": null, "meta": null, "type": "plan", "steps": [{"id": "m1", "kind": "action", "name": "memory_search", "inputs": {"query": "demo", "k": 3}, "preconditions": [], "postconditions": [], "retries": 0, "continue_on_error": false}, {"id": "s1", "kind": "action", "name": "python_exec", "inputs": {"code": "result = 40+2"}, "preconditions": [], "postconditions": ["result == 42"], "retries": 0, "continue_on_error": false}, {"id": "r1", "kind": "action", "name": "filesystem_read", "inputs": {"path": "server.py"}, "preconditions": [], "postconditions": ["len(result) > 0"], "retries": 0, "continue_on_error": false}], "ts": "2025-09-15T18:11:24.205088Z"}
